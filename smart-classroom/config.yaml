app:
  name: Smart Classroom
  cleanup_on_exit: true

monitoring:
  logs_dir: ./monitoring/logs
  interval: 2
models:
  asr:
    provider: openai # funasr and openai supported
    name: whisper-tiny # can be (whisper-tiny, whisper-small, whisper-medium ...) or (paraformer-zh etc)
    device: CPU # only CPU for whisper
    temperature: 0.0

  summarizer:
    provider: openvino
    name: Qwen/Qwen2-7B-Instruct # Qwen/Qwen1.5-7B-Chat, Qwen/Qwen2.5-7B-Instruct
    device: GPU # GPU or CPU
    weight_format: int8 # openvino supports fp16, fp32, int4, int8
    max_new_tokens: 1024
    temperature: 0.7 # 0.7 default
    models_base_path: "models"
    language: en # en or zh
    system_prompt: 
      en: "Summarize the classroom transcription in clear English. Preserve the full meaning of the lecture, focusing on key points and concepts. Do not add commentary or extra details beyond the transcription. Return the result in Markdown format"
      zh: "Summarize the classroom transcription in clear Mandarin. Preserve the full meaning of the lecture, focusing on key points and concepts. Do not add commentary or extra details beyond the transcription. Return the result in Markdown format"
    model_hub: huggingface # huggingface or modelscope

audio_preprocessing:
  chunk_duration_sec: 30
  silence_threshold: -35  # in dB
  silence_duration: 0.3   # minimum silence length in seconds
  search_window_sec: 1.5    # how far to look for silence if no silence exactly at chunk boundary
  chunk_output_path: chunks/

audio_util:
  max_size_mb: 200
  allowed_extensions:
    - .wav
    - .mp3
  chunk_size: 52428800   # 1024 * 1024 * 50 = 50MB

pipeline:
  delete_chunks_after_use: true
